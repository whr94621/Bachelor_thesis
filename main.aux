\relax 
\citation{harris1954distributional}
\citation{manning2008introduction}
\citation{tellex2003quantitative}
\citation{turian2010word}
\citation{socher2011parsing}
\citation{lund1996producing}
\citation{lebret2013word}
\@writefile{toc}{\contentsline {section}{\numberline {1}基本介绍}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}什么是词嵌入}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}之前的工作}{2}}
\citation{bengio2006neural}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{shazeer2016swivel}
\citation{levy2014neural}
\citation{levy2015improving}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  CBOW模型和skip-gram模型的大致框架\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:w2v}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}本文的工作}{5}}
\citation{vilnis2014word}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 79}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  词分布的示意图。这里选取的是选择高斯分布对词进行表示的情况。本图表现出了这种表示的两个特点：具有相近词义的分布的均值比较接近；词频较大和词义较宽泛的协方差较大。\relax }}{6}}
\newlabel{fig:w2d}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2}理论分析}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}模型介绍}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}分布的选取}{9}}
\citation{gomez2003survey}
\citation{nadarajah2005convolutions}
\citation{jebara2004probability}
\citation{jordan2003introduction}
\@writefile{toc}{\contentsline {section}{\numberline {3}词表示学习}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}学习过程}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}实验设置}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}结果及分析}{15}}
\@writefile{toc}{\contentsline {paragraph}{模型及超参数的选择}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  词表示实验的超参数设置\relax }}{16}}
\newlabel{tab:hy}{{1}{16}}
\@writefile{toc}{\contentsline {paragraph}{两种度量得到的结果}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  词分布表示在两种度量下得到的临近词的结果，其中IP表示用IP能量函数计算相似度，cosine表示用余弦函数只计算均值的相似度。IP能量函数得到的结果按照协方差矩阵的行列式由高到低排列。\relax }}{16}}
\@writefile{toc}{\contentsline {paragraph}{词义相似度评价}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  词相似度测评的结果。SG代表Skip-Gram模型，GR(Gaussian Representation)代表本文中的模型。50d表示维度为50.cos与IP分别指cosine函数和IP能量函数，从数据上看IP能量函数表现词相似度的能力相对较弱\relax }}{17}}
\newlabel{tab:sim}{{3}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4}上下位关系的学习}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}学习过程}{18}}
\citation{he2015learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  上下位词的两个例子\relax }}{19}}
\newlabel{fig:hyper}{{3}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}实验设置}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}结果及分析}{22}}
\@writefile{toc}{\contentsline {paragraph}{边际 $\varepsilon $ 、$\varepsilon ^{'}$的选取对于结果的影响}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  两种margin的选择对于结果的影响\relax }}{22}}
\@writefile{toc}{\contentsline {paragraph}{迭代次数对实验的影响}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  迭代次数对结果的影响\relax }}{23}}
\@writefile{toc}{\contentsline {paragraph}{两种模型的对照实验}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \fontsize  {10.5pt}{\baselineskip }\selectfont  模型对比实验\relax }}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {5}结论和展望}{24}}
\bibstyle{plain}
\bibdata{MyBib}
\bibcite{bengio2006neural}{1}
\bibcite{gomez2003survey}{2}
\bibcite{harris1954distributional}{3}
\bibcite{he2015learning}{4}
\bibcite{jebara2004probability}{5}
\bibcite{jordan2003introduction}{6}
\bibcite{lebret2013word}{7}
\bibcite{levy2014neural}{8}
\bibcite{levy2015improving}{9}
\bibcite{lund1996producing}{10}
\bibcite{manning2008introduction}{11}
\bibcite{mikolov2013efficient}{12}
\bibcite{nadarajah2005convolutions}{13}
\bibcite{pennington2014glove}{14}
\bibcite{shazeer2016swivel}{15}
\bibcite{socher2011parsing}{16}
\bibcite{tellex2003quantitative}{17}
\bibcite{turian2010word}{18}
\bibcite{vilnis2014word}{19}
